{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install fair-esm\n",
    "!pip install umap-learn\n",
    "!pip install \"umap-learn[plot]\"\n",
    "import pandas as pd\n",
    "\n",
    "file_url = \"https://raw.githubusercontent.com/txz32102/paper/main/data/drugminer/esm2_320_dimensions_with_labels.csv\"\n",
    "df = pd.read_csv(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, \\\n",
    "                    matthews_corrcoef, ConfusionMatrixDisplay,f1_score, \\\n",
    "                    accuracy_score, recall_score, precision_score, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import Module, Conv1d, Linear, Dropout, MaxPool1d, functional as F, BatchNorm1d, LazyLinear\n",
    "from torch.optim import Optimizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data = torch.from_numpy(x).float()\n",
    "        self.labels = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "def get_th_dataset(x, y):\n",
    "    \"\"\"\n",
    "    assemble a dataset with the given data and labels\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _dataset = CustomDataset(x, y)\n",
    "    return _dataset\n",
    "\n",
    "\n",
    "def get_dataloader(dataset: Dataset, batch_size):\n",
    "    \"\"\"\n",
    "    assemble a dataloader with the given dataset\n",
    "    :param dataset:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _dataLoader = DataLoader(dataset=dataset, batch_size=batch_size, pin_memory=True,\n",
    "                             drop_last=True, shuffle=True)\n",
    "    return _dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ASLSingleLabel(nn.Module):\n",
    "    '''\n",
    "    This loss is intended for single-label classification problems\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=0, gamma_neg=4, eps: float = 0.1, reduction='mean'):\n",
    "        super(ASLSingleLabel, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.targets_classes = []\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        '''\n",
    "        \"input\" dimensions: - (batch_size,number_classes)\n",
    "        \"target\" dimensions: - (batch_size)\n",
    "        '''\n",
    "        num_classes = inputs.size()[-1]\n",
    "        log_preds = self.logsoftmax(inputs)\n",
    "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1)\n",
    "\n",
    "        # ASL weights\n",
    "        targets = self.targets_classes\n",
    "        anti_targets = 1 - targets\n",
    "        xs_pos = torch.exp(log_preds)\n",
    "        xs_neg = 1 - xs_pos\n",
    "        xs_pos = xs_pos * targets\n",
    "        xs_neg = xs_neg * anti_targets\n",
    "        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n",
    "                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
    "        log_preds = log_preds * asymmetric_w\n",
    "\n",
    "        if self.eps > 0:  # label smoothing\n",
    "            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = - self.targets_classes.mul(log_preds)\n",
    "\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Bilstm(nn.Module):\n",
    "    def __init__(self, input_dim=320, hidden_dim=64, output_dim=2, num_layers=1, dropout=0):\n",
    "        super(Bilstm, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        x = x.unsqueeze(1)\n",
    "        # We need to unsqueeze the input tensor to match the batch_first=True setting\n",
    "        # Forward pass through LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # Extract the output of the last time step (many-to-one architecture)\n",
    "        out = out[:, -1, :].to(device)\n",
    "\n",
    "        # Pass the output through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # Apply softmax activation\n",
    "        out = torch.softmax(out, dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_log(epoch: int, loss: float, accuracy, logFile: str, is_append: bool):\n",
    "    info = str(epoch) + ' ' + str(loss) + ' ' + str(accuracy) + '\\n'\n",
    "    flag = 'a' if is_append else 'w'\n",
    "    file = open(logFile, flag)  # append mode\n",
    "    file.write(info)\n",
    "    file.close()\n",
    "\n",
    "def scores(y_pred: torch.Tensor, y_test: torch.Tensor):\n",
    "    predictions = torch.argmax(y_pred, dim=-1).detach().numpy()\n",
    "    labels = y_test.detach().numpy()\n",
    "    # labels = th.argmax(y_test, dim=-1).numpy()\n",
    "    recall = recall_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    precision = precision_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    f1 = f1_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    accuracy = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    # auc_score = roc_auc_score(y_score=y_pred.detach().numpy(), y_true=y_test.detach().numpy())\n",
    "    corr = matthews_corrcoef(y_true=labels, y_pred=predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true=labels, y_pred=predictions, )\n",
    "\n",
    "    report = {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        # \"auc\": auc_score,\n",
    "        'matthews_corrcoef': corr,\n",
    "        'balanced_accuracy': balanced_accuracy\n",
    "    }\n",
    "    return report\n",
    "\n",
    "\n",
    "def report(model: torch.nn.Module, dataset: CustomDataset):\n",
    "    _inputs, _labels = dataset.get_data().to(device), dataset.get_labels().to(device)\n",
    "    print(_inputs.size(0))\n",
    "    predictions = model(_inputs)\n",
    "    res = scores(predictions, _labels.squeeze())\n",
    "    print('accuracy ' + str(res[\"accuracy\"]))\n",
    "    print('precision ' + str(res[\"precision\"]))\n",
    "    print('f1 ' + str(res[\"f1\"]))\n",
    "    print('recall ' + str(res[\"recall\"]))\n",
    "    # print('auc_score ' + str(res[\"auc\"]))\n",
    "    print('matthews_corrcoef ' + str(res[\"matthews_corrcoef\"]))\n",
    "    print('balanced_accuracy ' + str(res[\"balanced_accuracy\"]))\n",
    "    # get_confusion_matrix(predictions, _labels.squeeze())\n",
    "\n",
    "\n",
    "def train(model: Module, EPOCHS, optimizer: Optimizer, criteria,\n",
    "           checkpoint, train_set: DataLoader, vali_set: DataLoader, device, LOG_VALIDATION, LOG_TRAIN):\n",
    "    \"\"\"\n",
    "    fine tune the model and save the best model in the checkpoint\n",
    "    :param LOG_TRAIN:\n",
    "    :param LOG_VALIDATION:\n",
    "    :param device:\n",
    "    :param model: a Cnn or ConvLSTM model\n",
    "    :param EPOCHS: hyperparameter Epoch\n",
    "    :param optimizer: pytorch optimizer\n",
    "    :param criteria: loss function\n",
    "    :param checkpoint: model checkpoint\n",
    "    :param train_set: a dataloader\n",
    "    :param vali_set: a dataloader\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if os.path.exists(LOG_VALIDATION):\n",
    "        os.remove(LOG_VALIDATION)\n",
    "    if os.path.exists(LOG_TRAIN):\n",
    "        os.remove(LOG_TRAIN)\n",
    "    model = model.to(device)\n",
    "    min_vali_loss = float(\"inf\")\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        running_loss = 0.0\n",
    "        train_acc = []\n",
    "        vali_loss = 0.0\n",
    "        model.train()\n",
    "        counter = 0\n",
    "        for i, (inputs, labels) in enumerate(train_set):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # outputs = outputs.squeeze()\n",
    "            loss = criteria(outputs.float(), labels.float().squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_acc.append(scores(outputs.to(\"cpu\"), labels.to(\"cpu\"))[\"accuracy\"])\n",
    "            counter = i\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        for j, (vali_inputs, vali_labels) in enumerate(vali_set):\n",
    "            vali_labels = vali_labels.to(device)\n",
    "            vali_inputs = vali_inputs.to(device)\n",
    "            vali_outputs = model(vali_inputs)\n",
    "            # vali_outputs = vali_outputs.squeeze()\n",
    "            acc = scores(vali_outputs.to('cpu'), vali_labels.to('cpu'))[\"accuracy\"]\n",
    "            vali_loss = criteria(vali_outputs.to(device).float(), vali_labels.to(device).float().squeeze())\n",
    "            if vali_loss < min_vali_loss:\n",
    "                min_vali_loss = vali_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "        avg_loss = running_loss / counter  # loss per batch\n",
    "        train_accuracy = sum(train_acc) / len(train_acc)\n",
    "        print('epoch {} train_loss: {} vali_loss: {} test_acc: {}'\n",
    "              .format(epoch + 1, f'{avg_loss:5f}', f'{vali_loss:5f}', f'{acc: 5f}'))\n",
    "        # logs\n",
    "        to_log(epoch, avg_loss, train_accuracy, LOG_TRAIN, True)\n",
    "        to_log(epoch, vali_loss.item(), acc, LOG_VALIDATION, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = os.path.join(os.getcwd(), 'Bilstm.pt')\n",
    "train_loss_log = os.path.join(os.getcwd(), \"Bilstm_train_log.txt\")\n",
    "test_loss_log = os.path.join(os.getcwd(), \"Bilstm_validation_log.txt\")\n",
    "\n",
    "plot_range = 10000  # range(1, 10000)\n",
    "stratify = True\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "epochs = 100\n",
    "weight_decay = 0\n",
    "\n",
    "X = df.drop(['label', 'UniProt_id'], axis=1)\n",
    "y = df['label'].apply(lambda x: 0 if x != 1 else x).to_numpy().astype(np.int64)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scalar = MinMaxScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "train_set = get_th_dataset(X_train, y_train)\n",
    "test_set = get_th_dataset(X_test, y_test)\n",
    "train_loader = get_dataloader(train_set, batch_size=batch_size)\n",
    "test_loader = get_dataloader(test_set, batch_size=len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Bilstm()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criteria = ASLSingleLabel(gamma_pos=1, gamma_neg=1, eps = 0.1)  # find the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train(model=model, EPOCHS=epochs, optimizer=optimizer, checkpoint=checkpoint, criteria=criteria,\n",
    "      train_set=train_loader, vali_set=test_loader, device=device, LOG_VALIDATION=test_loss_log, LOG_TRAIN=train_loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "checkpoint = os.path.join(os.getcwd(), 'Bilstm.pt')\n",
    "checkpoint = torch.load(checkpoint, map_location='cpu')\n",
    "saved_model = Bilstm()\n",
    "saved_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_th_dataset(X_train, y_train)\n",
    "test_set = get_th_dataset(X_test, y_test)\n",
    "print('train set val')\n",
    "report(saved_model, train_set)\n",
    "print('test set val')\n",
    "report(saved_model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc_curve(filename):\n",
    "    epochs, losses, accs = [], [], []\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            epoch, loss, acc = int(parts[0]), float(parts[1]), float(parts[2])\n",
    "            epochs.append(epoch)\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot Loss on the left y-axis\n",
    "    ax1.plot(epochs, losses, marker='o', linestyle='-', color='b', label='Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    \n",
    "    # Create a second y-axis for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, accs, marker='o', linestyle='-', color='r', label='Accuracy')\n",
    "    ax2.set_ylabel('Accuracy', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "\n",
    "    plt.title('Loss and Accuracy Curve')\n",
    "    fig.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
