{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 153,600\n",
      "number of parameters: 0.80M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65, 128)\n",
       "    (wpe): Embedding(320, 128)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=65, bias=False)\n",
       "  (classfication): Linear(in_features=65, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU, example:\n",
    "$ python train.py --batch_size=32 --compile=False\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT, CustomDataset, get_th_dataset, get_dataloader\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 320\n",
    "# model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 128\n",
    "vocab_size=65\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda:0' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "\n",
    "index_X_train = 0\n",
    "df = pd.read_csv('../data/drugfinder/esm2_320_dimensions_with_labels.csv') \n",
    "X = df.drop(['label', 'UniProt_id'], axis=1)\n",
    "y = df['label'].apply(lambda x: 0 if x != 1 else x).to_numpy().astype(np.int64)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scalar = MinMaxScaler(feature_range=(0, 64))\n",
    "# Fit and transform X_train\n",
    "X_train = np.round(scalar.fit_transform(X_train)).astype(int)\n",
    "# Transform X_test using the same scaler and round to integers\n",
    "X_test = np.round(scalar.fit_transform(X_test)).astype(int)\n",
    "\n",
    "def get_batch(split):\n",
    "    global index_X_train\n",
    "    batch_size = 12\n",
    "    num_samples = len(X_train)\n",
    "    \n",
    "    if index_X_train + batch_size >= num_samples:\n",
    "        # Wrap back to 0 when reaching the end of the data\n",
    "        batch = X_train[index_X_train: num_samples]\n",
    "        index_X_train = 0\n",
    "    else:\n",
    "        # Get a batch without wrapping\n",
    "        batch = X_train[index_X_train: index_X_train + batch_size]\n",
    "        index_X_train += batch_size\n",
    "\n",
    "    return batch\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=vocab_size, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, \\\n",
    "                    matthews_corrcoef, ConfusionMatrixDisplay,f1_score, \\\n",
    "                    accuracy_score, recall_score, precision_score, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import Module, Conv1d, Linear, Dropout, MaxPool1d, functional as F, BatchNorm1d, LazyLinear\n",
    "from torch.optim import Optimizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_log(epoch: int, loss: float, accuracy, logFile: str, is_append: bool):\n",
    "    info = str(epoch) + ' ' + str(loss) + ' ' + str(accuracy) + '\\n'\n",
    "    flag = 'a' if is_append else 'w'\n",
    "    file = open(logFile, flag)  # append mode\n",
    "    file.write(info)\n",
    "    file.close()\n",
    "\n",
    "def scores(y_pred: torch.Tensor, y_test: torch.Tensor):\n",
    "    predictions = torch.argmax(y_pred, dim=-1).numpy()\n",
    "    labels = y_test.numpy()\n",
    "    # labels = th.argmax(y_test, dim=-1).numpy()\n",
    "    recall = recall_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    precision = precision_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    f1 = f1_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    accuracy = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    # auc_score = roc_auc_score(y_score=y_pred.detach().numpy(), y_true=y_test.detach().numpy())\n",
    "    corr = matthews_corrcoef(y_true=labels, y_pred=predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true=labels, y_pred=predictions, )\n",
    "\n",
    "    report = {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        # \"auc\": auc_score,\n",
    "        'matthews_corrcoef': corr,\n",
    "        'balanced_accuracy': balanced_accuracy\n",
    "    }\n",
    "    return report\n",
    "\n",
    "\n",
    "def report(model: torch.nn.Module, dataset: CustomDataset):\n",
    "    _inputs, _labels = dataset.get_data(), dataset.get_labels()\n",
    "    print(_inputs.size(0))\n",
    "    predictions = model(_inputs)\n",
    "    res = scores(predictions, _labels.squeeze())\n",
    "    print('accuracy ' + str(res[\"accuracy\"]))\n",
    "    print('precision ' + str(res[\"precision\"]))\n",
    "    print('f1 ' + str(res[\"f1\"]))\n",
    "    print('recall ' + str(res[\"recall\"]))\n",
    "    # print('auc_score ' + str(res[\"auc\"]))\n",
    "    print('matthews_corrcoef ' + str(res[\"matthews_corrcoef\"]))\n",
    "    print('balanced_accuracy ' + str(res[\"balanced_accuracy\"]))\n",
    "    # get_confusion_matrix(predictions, _labels.squeeze())\n",
    "\n",
    "\n",
    "def train(model: Module, cur_epoch: int, optimizer: Optimizer, criteria,\n",
    "           checkpoint, train_set: DataLoader, vali_set: DataLoader, min_vali_loss: float, device, LOG_VALIDATION, LOG_TRAIN):\n",
    "    \"\"\"\n",
    "    fine tune the model and save the best model in the checkpoint\n",
    "    :param LOG_TRAIN:\n",
    "    :param LOG_VALIDATION:\n",
    "    :param device:\n",
    "    :param model: a Cnn or ConvLSTM model\n",
    "    :param EPOCHS: hyperparameter Epoch\n",
    "    :param optimizer: pytorch optimizer\n",
    "    :param criteria: loss function\n",
    "    :param checkpoint: model checkpoint\n",
    "    :param train_set: a dataloader\n",
    "    :param vali_set: a dataloader\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    running_loss = 0.0\n",
    "    train_acc = []\n",
    "    vali_loss = 0.0\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    for i, (inputs, labels) in enumerate(train_set):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # outputs = outputs.squeeze()\n",
    "        loss = criteria(outputs.float(), labels.float().squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        train_acc.append(scores(outputs.to(\"cpu\"), labels.to(\"cpu\"))[\"accuracy\"])\n",
    "        counter = i\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    for j, (vali_inputs, vali_labels) in enumerate(vali_set):\n",
    "        vali_labels = vali_labels.to(device)\n",
    "        vali_inputs = vali_inputs.to(device)\n",
    "        vali_outputs = model(vali_inputs)\n",
    "        # vali_outputs = vali_outputs.squeeze()\n",
    "        acc = scores(vali_outputs.to('cpu'), vali_labels.to('cpu'))[\"accuracy\"]\n",
    "        vali_loss = criteria(vali_outputs.to(device).float(), vali_labels.to(device).float().squeeze())\n",
    "        if vali_loss < min_vali_loss:\n",
    "            min_vali_loss = vali_loss\n",
    "            torch.save({\n",
    "                'epoch': cur_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint)\n",
    "    avg_loss = running_loss / counter  # loss per batch\n",
    "    train_accuracy = sum(train_acc) / len(train_acc)\n",
    "    print('epoch {} train_loss: {} vali_loss: {} test_acc: {}'\n",
    "            .format(cur_epoch + 1, f'{avg_loss:5f}', f'{vali_loss:5f}', f'{acc: 5f}'))\n",
    "    # logs\n",
    "    to_log(cur_epoch, avg_loss, train_accuracy, LOG_TRAIN, True)\n",
    "    to_log(cur_epoch, vali_loss.item(), acc, LOG_VALIDATION, True)\n",
    "    return vali_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLSingleLabel(nn.Module):\n",
    "    '''\n",
    "    This loss is intended for single-label classification problems\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=0, gamma_neg=4, eps: float = 0.1, reduction='mean'):\n",
    "        super(ASLSingleLabel, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.targets_classes = []\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        '''\n",
    "        \"input\" dimensions: - (batch_size,number_classes)\n",
    "        \"target\" dimensions: - (batch_size)\n",
    "        '''\n",
    "        num_classes = inputs.size()[-1]\n",
    "        log_preds = self.logsoftmax(inputs)\n",
    "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1)\n",
    "\n",
    "        # ASL weights\n",
    "        targets = self.targets_classes\n",
    "        anti_targets = 1 - targets\n",
    "        xs_pos = torch.exp(log_preds)\n",
    "        xs_neg = 1 - xs_pos\n",
    "        xs_pos = xs_pos * targets\n",
    "        xs_neg = xs_neg * anti_targets\n",
    "        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n",
    "                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
    "        log_preds = log_preds * asymmetric_w\n",
    "\n",
    "        if self.eps > 0:  # label smoothing\n",
    "            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = - self.targets_classes.mul(log_preds)\n",
    "\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 17:00:03.441682: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-29 17:00:03.463007: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-29 17:00:03.463027: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-29 17:00:03.463040: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-29 17:00:03.467584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = os.path.join(os.getcwd(), 'gpt.pt')\n",
    "loss_log = os.path.join(os.getcwd(), \"gpt_log.txt\")\n",
    "\n",
    "plot_range = 10000  # range(1, 10000)\n",
    "stratify = True\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "epochs = 1\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criteria = ASLSingleLabel(gamma_pos=1, gamma_neg=1, eps = 0.1)  # find the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train_loss: 0.128271 vali_loss: 0.197233 test_acc:  0.873913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_loss = float(\"inf\")\n",
    "loss_list = [float(\"inf\")]\n",
    "for i in range(500):\n",
    "      train_set = get_th_dataset(X_train, y_train)\n",
    "      test_set = get_th_dataset(X_test, y_test)\n",
    "      train_loader = get_dataloader(train_set, batch_size=batch_size)\n",
    "      test_loader = get_dataloader(test_set, batch_size=len(test_set))\n",
    "      current_loss = train(model=model, cur_epochs=i, optimizer=optimizer, checkpoint=checkpoint, criteria=criteria,\n",
    "            train_set=train_loader, vali_set=test_loader, device=device, LOG_TRAIN=loss_log, min_vali_loss=min_loss)\n",
    "      loss_list.append(current_loss)\n",
    "      min_loss = min(loss_list)\n",
    "      if((i + 1) % 10 == 0):\n",
    "            print(f\"current epoch is {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
