{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# esm_fine-tuning_embedding.pt is for embedding 320 dimensional data\n","# esm_fine-tuning.pt is the classification model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install fair-esm\n","# !pip install Bio\n","# negative_train_path = \"/kaggle/input/fastadata/Train/negative_train_sequence.fasta\"\n","# positive_train_path = \"/kaggle/input/fastadata/Train/positive_train_sequence.fasta\"\n","# negative_test_path = \"/kaggle/input/fastadata/Independent_Test/negative_test_sequence.fasta\"\n","# positive_test_path = \"/kaggle/input/fastadata/Independent_Test/positive_test_sequence.fasta\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T08:05:07.601428Z","iopub.status.busy":"2023-12-05T08:05:07.601085Z","iopub.status.idle":"2023-12-05T08:05:11.879188Z","shell.execute_reply":"2023-12-05T08:05:11.878380Z","shell.execute_reply.started":"2023-12-05T08:05:07.601397Z"},"id":"5k37ZxXv3eys","trusted":true},"outputs":[],"source":["# Standard Python libraries\n","import os\n","import copy\n","import warnings\n","import random\n","\n","# Data manipulation and analysis\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.utils import compute_class_weight\n","from sklearn.metrics import (confusion_matrix, roc_auc_score, matthews_corrcoef,\n","                             ConfusionMatrixDisplay, f1_score, accuracy_score,\n","                             recall_score, precision_score, balanced_accuracy_score,\n","                             roc_curve)\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","\n","# PyTorch and related modules\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import Optimizer, Adam\n","from torch.nn import (Module, Conv1d, Linear, Dropout, MaxPool1d, functional as F, \n","                      BatchNorm1d, LazyLinear)\n","\n","# ESM and related modules\n","import esm\n","from esm.modules import ContactPredictionHead, ESM1bLayerNorm, RobertaLMHead, TransformerLayer\n","\n","# tqdm for progress bars\n","from tqdm import tqdm\n","\n","from Bio import SeqIO\n","from typing import Union"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T08:05:15.347959Z","iopub.status.busy":"2023-12-05T08:05:15.347491Z","iopub.status.idle":"2023-12-05T08:05:15.561498Z","shell.execute_reply":"2023-12-05T08:05:15.560392Z","shell.execute_reply.started":"2023-12-05T08:05:15.347930Z"},"id":"slPEM1_WACMw","trusted":true},"outputs":[],"source":["class esm2_t6_8M_UR50D(nn.Module):\n","    def __init__(\n","        self,\n","        num_layers: int = 6,\n","        embed_dim: int = 320,  # Change the embed_dim to match the ESM2 module\n","        attention_heads: int = 4,  # Change the attention_heads to match the ESM2 module\n","        alphabet: Union[esm.data.Alphabet, str] = \"ESM-1b\",\n","        token_dropout: bool = True,\n","        linear_in1: int = 320,\n","        linear_in2: int = 180,\n","        linear_in3: int = 60,\n","        linear_in4: int = 30,\n","        linear_out: int = 1\n","    ):\n","        super().__init__()\n","        _, self.alphabet_ = esm.pretrained.esm2_t6_8M_UR50D()\n","        self.num_layers = num_layers\n","        self.embed_dim = embed_dim\n","        self.attention_heads = attention_heads\n","        if not isinstance(alphabet, esm.data.Alphabet):\n","            alphabet = esm.data.Alphabet.from_architecture(alphabet)\n","        self.alphabet = alphabet\n","        self.alphabet_size = len(alphabet)\n","        self.padding_idx = alphabet.padding_idx\n","        self.mask_idx = alphabet.mask_idx\n","        self.cls_idx = alphabet.cls_idx\n","        self.eos_idx = alphabet.eos_idx\n","        self.prepend_bos = alphabet.prepend_bos\n","        self.append_eos = alphabet.append_eos\n","        self.token_dropout = token_dropout\n","        self.layer1 = nn.Linear(linear_in1, linear_in2)\n","        self.act1 = nn.ReLU()\n","        self.layer2 = nn.Linear(linear_in2, linear_in3)\n","        self.act2 = nn.ReLU()\n","        self.layer3 = nn.Linear(linear_in3, linear_in4)\n","        self.act3 = nn.ReLU()\n","        self.output = nn.Linear(linear_in4, linear_out)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.batch_converter = alphabet.get_batch_converter()\n","        self._init_submodules()\n","\n","    def _init_submodules(self):\n","        self.embed_scale = 1\n","        self.embed_tokens = nn.Embedding(\n","            self.alphabet_size,\n","            self.embed_dim,\n","            padding_idx=self.padding_idx,\n","        )\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerLayer(\n","                    self.embed_dim,\n","                    4 * self.embed_dim,\n","                    self.attention_heads,\n","                    add_bias_kv=False,\n","                    use_esm1b_layer_norm=True,\n","                    use_rotary_embeddings=True,\n","                )\n","                for _ in range(self.num_layers)\n","            ]\n","        )\n","\n","        self.contact_head = ContactPredictionHead(\n","            self.num_layers * self.attention_heads,\n","            self.prepend_bos,\n","            self.append_eos,\n","            eos_idx=self.eos_idx,\n","        )\n","        self.emb_layer_norm_after = ESM1bLayerNorm(self.embed_dim)\n","\n","        self.lm_head = RobertaLMHead(\n","            embed_dim=self.embed_dim,\n","            output_dim=self.alphabet_size,\n","            weight=self.embed_tokens.weight,\n","        )\n","\n","    def forward(self, tokens, repr_layers=[6], need_head_weights=False, return_contacts=True):\n","        batch_lens = (tokens != self.alphabet_.padding_idx).sum(1)\n","        if return_contacts:\n","            need_head_weights = True\n","\n","        assert tokens.ndim == 2\n","        padding_mask = tokens.eq(self.padding_idx)  # B, T\n","\n","        x = self.embed_scale * self.embed_tokens(tokens)\n","\n","        if self.token_dropout:\n","            x.masked_fill_((tokens == self.mask_idx).unsqueeze(-1), 0.0)\n","            mask_ratio_train = 0.15 * 0.8\n","            src_lengths = (~padding_mask).sum(-1)\n","            mask_ratio_observed = (tokens == self.mask_idx).sum(-1).to(x.dtype) / src_lengths\n","            x = x * (1 - mask_ratio_train) / (1 - mask_ratio_observed)[:, None, None]\n","\n","        if padding_mask is not None:\n","            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n","\n","        repr_layers = set(repr_layers)\n","        hidden_representations = {}\n","        if 0 in repr_layers:\n","            hidden_representations[0] = x\n","\n","        if need_head_weights:\n","            attn_weights = []\n","\n","        x = x.transpose(0, 1)\n","\n","        if not padding_mask.any():\n","            padding_mask = None\n","\n","        for layer_idx, layer in enumerate(self.layers):\n","            x, attn = layer(\n","                x,\n","                self_attn_padding_mask=padding_mask,\n","                need_head_weights=need_head_weights,\n","            )\n","            if (layer_idx + 1) in repr_layers:\n","                hidden_representations[layer_idx + 1] = x.transpose(0, 1)\n","            if need_head_weights:\n","                attn_weights.append(attn.transpose(1, 0))\n","\n","        x = self.emb_layer_norm_after(x)\n","        x = x.transpose(0, 1)\n","\n","        if (layer_idx + 1) in repr_layers:\n","            hidden_representations[layer_idx + 1] = x\n","        x = self.lm_head(x)\n","\n","        result = {\"logits\": x, \"representations\": hidden_representations}\n","        if need_head_weights:\n","            attentions = torch.stack(attn_weights, 1)\n","            if padding_mask is not None:\n","                attention_mask = 1 - padding_mask.type_as(attentions)\n","                attention_mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2)\n","                attentions = attentions * attention_mask[:, None, None, :, :]\n","            result[\"attentions\"] = attentions\n","            if return_contacts:\n","                contacts = self.contact_head(tokens, attentions)\n","                result[\"contacts\"] = contacts\n","        x = result[\"representations\"][6]\n","        sequence_representations = []\n","        for i, tokens_len in enumerate(batch_lens):\n","            sequence_representations.append(x[i, 1 : tokens_len - 1].mean(0))\n","        x = torch.stack(sequence_representations)\n","        x = self.act1(self.layer1(x))\n","        x = self.act2(self.layer2(x))\n","        x = self.act3(self.layer3(x))\n","        x = self.sigmoid(self.output(x))\n","        return x\n","        \n","def read_fasta(file_path, type):\n","    \"\"\"Reads a FASTA file and returns a list of tuples with protein labels (1 or 0) and sequences, filtering out sequences less than 100 and more than 1000 characters long.\"\"\"\n","    data = []\n","    for record in SeqIO.parse(file_path, \"fasta\"):\n","        protein_name = record.id\n","        sequence = str(record.seq)\n","\n","        # Determine the protein_label based on the 'type' parameter\n","        if type == '1':\n","            protein_label = 1\n","        else:\n","            protein_label = 0\n","\n","        # Check if the sequence length is within the desired range\n","        if 0 <= len(sequence) <= 2000:\n","            data.append((protein_label, sequence))\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T08:05:18.883987Z","iopub.status.busy":"2023-12-05T08:05:18.883156Z","iopub.status.idle":"2023-12-05T08:05:19.116400Z","shell.execute_reply":"2023-12-05T08:05:19.115221Z","shell.execute_reply.started":"2023-12-05T08:05:18.883951Z"},"id":"0a7MREjTADir","trusted":true},"outputs":[],"source":["negative_train_path = \"./data/drugminer/fastadata/Train/negative_train_sequence.fasta\"\n","positive_train_path = \"./data/drugminer/fastadata/Train/positive_train_sequence.fasta\"\n","negative_test_path = \"./data/drugminer/fastadata/Independent_Test/negative_test_sequence.fasta\"\n","positive_test_path = \"./data/drugminer/fastadata/Independent_Test/positive_test_sequence.fasta\"\n","# Extracting data\n","negative_train_data = read_fasta(negative_train_path, '0')\n","positive_train_data = read_fasta(positive_train_path, '1')\n","negative_test_data = read_fasta(negative_test_path, '0')\n","positive_test_data = read_fasta(positive_test_path, '1')\n","\n","negative_data = negative_train_data + negative_test_data\n","\n","positive_data = positive_train_data + positive_test_data\n","\n","if len(negative_data) > len(positive_data):\n","    negative_data = negative_data[:len(positive_data)]\n","elif len(positive_data) > len(negative_data):\n","    positive_data = positive_data[:len(negative_data)]\n","\n","data = negative_data + positive_data\n","\n","_, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n","batch_converter = alphabet.get_batch_converter()\n","\n","batch_labels, batch_strs, batch_tokens = batch_converter(data)\n","batch_labels_tensor = torch.tensor(batch_labels)\n","X_train, X_test, y_train, y_test = train_test_split(batch_tokens, batch_labels_tensor, test_size=0.2, random_state=42)\n","\n","warnings.filterwarnings(\"ignore\", category=Warning)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","model = esm2_t6_8M_UR50D()\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T08:07:07.654855Z","iopub.status.busy":"2023-12-05T08:07:07.654473Z","iopub.status.idle":"2023-12-05T08:07:07.856679Z","shell.execute_reply":"2023-12-05T08:07:07.855670Z","shell.execute_reply.started":"2023-12-05T08:07:07.654817Z"},"id":"eVC2R1AtAiht","trusted":true},"outputs":[],"source":["# Helper function to train one model\n","def model_train(model, X_train, y_train, X_test, y_test):\n","    X_train = torch.cat([X_train, X_test], dim=0)\n","    y_train = torch.cat([y_train, y_test], dim=0)\n","    model = model.to(device)\n","    X_train = X_train.to(device)\n","    y_train = y_train.to(device).float()\n","    X_test = X_test.to(device)\n","    y_test = y_test.to(device).float()\n","    # loss function and optimizer\n","    loss_fn = nn.BCELoss()  # binary cross entropy\n","    optimizer = Adam(model.parameters(), lr=0.0001)\n","\n","    n_epochs = 3   # number of epochs to run\n","    batch_size = 3  # size of each batch\n","    batch_start = torch.arange(0, len(X_train), batch_size)\n","\n","    # Hold the best model\n","    best_acc = - np.inf   # init to negative infinity\n","    best_weights = None\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        with tqdm(batch_start, unit=\"batch\", mininterval=0, disable=False) as bar:\n","            bar.set_description(f\"Epoch {epoch}\")\n","            for start in bar:\n","                # take a batch\n","                X_batch = X_train[start:start+batch_size]\n","                y_batch = y_train[start:start+batch_size]\n","                # forward pass\n","                y_pred = model(X_batch).reshape(-1)\n","                loss = loss_fn(y_pred, y_batch)\n","                # backward pass\n","                optimizer.zero_grad()\n","                loss.backward()\n","                # update weights\n","                optimizer.step()\n","                # print progress\n","    torch.save(model.state_dict(), 'esm_fine-tuning.pt')\n","    return model\n","\n","def validate_model(model, batch_labels, batch_strs, batch_tokens, device):\n","    model.eval()  # Set the model to evaluation mode\n","\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():  # Disable gradient calculations for inference\n","        for i in range(0, len(batch_tokens), 2):  # Assuming batch size of 10\n","            # Get the current batch\n","            c = batch_tokens[i:i + 2].to(device)\n","\n","            # Forward pass\n","            res = model(c)\n","\n","            # Convert to binary predictions\n","            binary_res = (res > 0.5).int()\n","\n","            # Store predictions and true labels\n","            predictions.extend(binary_res.cpu().tolist())  # Move to CPU if necessary\n","            true_labels.extend(batch_labels[i:i + 2])\n","\n","    return predictions, true_labels\n","\n","def calculate_metrics(true_labels, predictions):\n","    # Flatten predictions if they are a list of lists\n","    if predictions and isinstance(predictions[0], list):\n","        predictions = [item for sublist in predictions for item in sublist]\n","\n","    # Convert predictions and true_labels to the correct format\n","    true_labels = [int(label) for label in true_labels]\n","    predictions = [int(round(pred)) for pred in predictions]\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(true_labels, predictions)\n","    f1 = f1_score(true_labels, predictions)\n","    recall = recall_score(true_labels, predictions)\n","    mcc = matthews_corrcoef(true_labels, predictions)\n","\n","    # Confusion matrix to calculate sensitivity (True Positive Rate)\n","    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n","    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n","\n","    return {\n","        \"Accuracy\": accuracy,\n","        \"F1 Score\": f1,\n","        \"Recall\": recall,\n","        \"Sensitivity\": sensitivity,\n","        \"MCC\": mcc\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T08:07:14.705664Z","iopub.status.busy":"2023-12-05T08:07:14.704957Z"},"trusted":true},"outputs":[],"source":["# this is the fintuing model\n","state_dict = torch.load('esm_fine-tuning.pt')\n","\n","# Load the weights into model3\n","model.load_state_dict(state_dict)\n","\n","for i in range(30):\n","    model = model_train(model, X_train, y_train, X_test, y_test)\n","    predictions, true_labels = validate_model(model, batch_labels, batch_strs, batch_tokens, device)\n","    metrics = calculate_metrics(true_labels, predictions)\n","    for metric, value in metrics.items():\n","        print(f\"{metric}: {value:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:03:11.360082Z","iopub.status.idle":"2023-12-05T08:03:11.360439Z","shell.execute_reply":"2023-12-05T08:03:11.360281Z","shell.execute_reply.started":"2023-12-05T08:03:11.360264Z"},"id":"kmcnGBwVa1Hk","trusted":true},"outputs":[],"source":["# Assume model_ and model are your pre-defined models\n","\n","# Step 1: Extract weights from model_\n","source_state_dict = model.state_dict()\n","\n","\n","embedding_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n","# Step 2: Prepare a new state dict for the target model\n","target_state_dict = embedding_model.state_dict()\n","\n","# Step 3: Copy weights from the source model to the target model for matching layers\n","for name, param in source_state_dict.items():\n","    if name in target_state_dict:\n","        try:\n","            # Ensure the shapes are the same, otherwise skip\n","            if param.shape == target_state_dict[name].shape:\n","                target_state_dict[name].copy_(param)\n","            else:\n","                print(f\"Shape mismatch at {name}, skipping\")\n","        except Exception as e:\n","            print(f\"Failed to copy: {e}\")\n","\n","# Optionally, load the modified state dict back into the model\n","embedding_model.load_state_dict(target_state_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-05T08:03:11.363178Z","iopub.status.idle":"2023-12-05T08:03:11.363519Z","shell.execute_reply":"2023-12-05T08:03:11.363368Z","shell.execute_reply.started":"2023-12-05T08:03:11.363352Z"},"trusted":true},"outputs":[],"source":["torch.save(embedding_model.state_dict(), 'esm_fine-tuning_embedding.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNPvVRfv1bC5SBWLjw4JsWd","gpuType":"T4","private_outputs":true,"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4107058,"sourceId":7120675,"sourceType":"datasetVersion"},{"datasetId":4111731,"sourceId":7127416,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
